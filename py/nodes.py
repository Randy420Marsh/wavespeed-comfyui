from .wavespeed_api import WaveSpeedAPI
import base64
import io
import os
import re
import numpy
import PIL
import requests
import torch
from collections.abc import Iterable
import configparser
import folder_paths
import time
from typing import List, Dict, Any, Tuple, Optional, Union

try:
    current_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(current_dir)
    config_path = os.path.join(parent_dir, 'config.ini')
    config = configparser.ConfigParser()

    if not os.path.exists(config_path):
        config['API'] = {'WAVESPEED_API_KEY': ''}
        with open(config_path, 'w') as config_file:
            config.write(config_file)

    config.read(config_path)
except Exception as e:
    print(f"Error reading or creating config file: {e}")
    config = None


def _fetch_image(url, stream=True):
    return requests.get(url, stream=stream).content


def _tensor2images(tensor):
    np_imgs = numpy.clip(tensor.cpu().numpy() * 255.0, 0.0, 255.0).astype(numpy.uint8)
    return [PIL.Image.fromarray(np_img) for np_img in np_imgs]


def _images2tensor(images):
    if isinstance(images, Iterable):
        return torch.stack([torch.from_numpy(numpy.array(image)).float() / 255.0 for image in images])
    return torch.from_numpy(numpy.array(images)).unsqueeze(0).float() / 255.0


def _decode_image(data_bytes, rtn_mask=False):
    with io.BytesIO(data_bytes) as bytes_io:
        img = PIL.Image.open(bytes_io)
        if not rtn_mask:
            img = img.convert('RGB')
        elif 'A' in img.getbands():
            img = img.getchannel('A')
        else:
            img = None
    return img


def _encode_image(img, mask=None):
    if mask is not None:
        img = img.copy()
        img.putalpha(mask)
    with io.BytesIO() as bytes_io:
        if mask is not None:
            img.save(bytes_io, format='PNG')
        else:
            img.save(bytes_io, format='JPEG')
        data_bytes = bytes_io.getvalue()
    return data_bytes


def _image_to_base64(image):
    if image is None:
        return None
    return base64.b64encode(_encode_image(_tensor2images(image)[0])).decode("utf-8")

class WaveSpeedAIAPIClient:
    """
    WaveSpeed AI API Client Node

    This node creates a client for connecting to the WaveSpeed AI API.
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "api_key": ("STRING", {"multiline": False, "default": ""}),
            },
        }

    RETURN_TYPES = ("WAVESPEED_AI_API_CLIENT",)
    RETURN_NAMES = ("client",)

    FUNCTION = "create_client"

    CATEGORY = "WaveSpeedAI"

    def create_client(self, api_key):
        """
        Create a WaveSpeed AI API client

        Args:
            api_key: WaveSpeed AI API key

        Returns:
            WaveSpeedAPI: WaveSpeed AI API client
        """
        if api_key == "":
            try:
                wavespeed_api_key = config['API']['WAVESPEED_API_KEY']
                if wavespeed_api_key == '':
                    raise ValueError('API_KEY is empty')

            except KeyError:
                raise ValueError('Unable to find API_KEY in config.ini')

            client = WaveSpeedAPI(wavespeed_api_key)
        else:
            client = WaveSpeedAPI(api_key)

        return (client,)


class PreviewVideo:
    """
    Preview Video Node for ComfyUI

    This node allows previewing and saving videos generated by WaveSpeed AI.
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "video_url": ("STRING", {"forceInput": True}),
                "filename_prefix": ("STRING", {"default": "wavespeed_video"}),
            }
        }

    OUTPUT_NODE = True
    FUNCTION = "run"
    CATEGORY = "WaveSpeedAI"
    RETURN_TYPES = ()
    RETURN_NAMES = ()

    def run(self, video_url, filename_prefix):
        """
        Preview and save a video

        Args:
            video_url: URL of the video
            filename_prefix: Prefix for the saved file

        Returns:
            str: Path to the saved video file
        """
        if not video_url:
            raise ValueError("No video URL provided")

        try:
            response = requests.get(video_url)
            response.raise_for_status()
            video_data = response.content
        except requests.RequestException as e:
            raise RuntimeError(f"Error downloading video: {e}")

        output_dir = folder_paths.get_output_directory()
        filename = f"{filename_prefix}_{int(time.time())}.mp4"
        file_path = os.path.join(output_dir, filename)

        with open(file_path, "wb") as f:
            f.write(video_data)

        return {"ui": {"video_url": [video_url]}, "result": ('',)}


class WanLoras:
    """
    WaveSpeed AI LoRA Parameters Node

    This node is used to set LoRA model parameters for use by other nodes.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "lora1_path": ("STRING", {"multiline": False, "default": ""}),
                "lora1_scale": ("FLOAT", {
                    "default": 0.8,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "display": "number"
                }),
            },
            "optional": {
                "lora2_path": ("STRING", {"multiline": False, "default": ""}),
                "lora2_scale": ("FLOAT", {
                    "default": 0.8,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "display": "number"
                }),
                "lora3_path": ("STRING", {"multiline": False, "default": ""}),
                "lora3_scale": ("FLOAT", {
                    "default": 0.8,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "display": "number"
                }),
            }
        }

    RETURN_TYPES = ("WAVESPEED_WAN_LORAS",)
    RETURN_NAMES = ("loras",)

    FUNCTION = "create_loras"

    CATEGORY = "WaveSpeedAI"

    def create_loras(self,
                     lora1_path,
                     lora1_scale,
                     lora2_path="",
                     lora2_scale=0.8,
                     lora3_path="",
                     lora3_scale=0.8):
        """
        Create a list of LoRA parameters

        Args:
            lora1_path: Path to the first LoRA model
            lora1_scale: Weight of the first LoRA model
            lora2_path: Path to the second LoRA model
            lora2_scale: Weight of the second LoRA model
            lora3_path: Path to the third LoRA model
            lora3_scale: Weight of the third LoRA model
        Returns:
            list: List of LoRA parameters
        """
        loras = []

        # Add non-empty LoRA parameters
        for path, scale in [(lora1_path, lora1_scale),
                            (lora2_path, lora2_scale),
                            (lora3_path, lora3_scale)]:
            if path.strip():
                loras.append({"path": path.strip(), "scale": float(scale)})

        return (loras,)


class FluxLoras:
    """
    WaveSpeed AI LoRA Parameter Node

    This node is used to set LoRA model parameters for use by other nodes.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "lora1_path": ("STRING", {"multiline": False, "default": ""}),
                "lora1_scale": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.0,
                    "max": 4.0,
                    "step": 0.05,
                    "display": "number"
                }),
            },
            "optional": {
                "lora2_path": ("STRING", {"multiline": False, "default": ""}),
                "lora2_scale": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.0,
                    "max": 4.0,
                    "step": 0.05,
                    "display": "number"
                }),
                "lora3_path": ("STRING", {"multiline": False, "default": ""}),
                "lora3_scale": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.0,
                    "max": 4.0,
                    "step": 0.05,
                    "display": "number"
                }),
                "lora4_path": ("STRING", {"multiline": False, "default": ""}),
                "lora4_scale": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.0,
                    "max": 4.0,
                    "step": 0.05,
                    "display": "number"
                }),
                "lora5_path": ("STRING", {"multiline": False, "default": ""}),
                "lora5_scale": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.0,
                    "max": 4.0,
                    "step": 0.05,
                    "display": "number"
                }),
            }
        }

    RETURN_TYPES = ("WAVESPEED_FLUX_LORAS",)
    RETURN_NAMES = ("loras",)

    FUNCTION = "create_loras"

    CATEGORY = "WaveSpeedAI"

    def create_loras(self,
                     lora1_path,
                     lora1_scale,
                     lora2_path="",
                     lora2_scale=1.0,
                     lora3_path="",
                     lora3_scale=1.0,
                     lora4_path="",
                     lora4_scale=1.0,
                     lora5_path="",
                     lora5_scale=1.0):
        """
        Create a list of LoRA parameters

        Args:
            lora1_path: Path to the first LoRA model
            lora1_scale: Weight of the first LoRA model
            lora2_path: Path to the second LoRA model
            lora2_scale: Weight of the second LoRA model
            lora3_path: Path to the third LoRA model
            lora3_scale: Weight of the third LoRA model
            lora4_path: Path to the fourth LoRA model
            lora4_scale: Weight of the fourth LoRA model
            lora5_path: Path to the fifth LoRA model
            lora5_scale: Weight of the fifth LoRA model

        Returns:
            list: List of LoRA parameters
        """
        loras = []

        # Add non-empty LoRA parameters
        for path, scale in [(lora1_path, lora1_scale),
                            (lora2_path, lora2_scale),
                            (lora3_path, lora3_scale),
                            (lora4_path, lora4_scale),
                            (lora5_path, lora5_scale)]:
            if path.strip():
                loras.append({"path": path.strip(), "scale": float(scale)})

        return (loras,)


class FluxImage2Image:
    """
    Flux Image Generator Node

    This node uses WaveSpeed AI's Flux model to generate high-quality images.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "client": ("WAVESPEED_AI_API_CLIENT",),
                "model": (["flux-dev", "flux-schnell"], {"tooltip": "Model name, choose from flux-dev or flux-schnell"}),
                "image_url": ("STRING", {"multiline": False, "default": "", "tooltip": "Image URL for image-to-image generation"}),
                "strength": ("FLOAT", {
                    "default": 0.6,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.1,
                    "display": "number",
                    "tooltip": "Strength of the image-to-image transformation (0.0 to 1.0)"
                }),
            },
            "optional": {
                "prompt": ("STRING", {"multiline": True, "default": "", "tooltip": "Text description of the image to generate"}),
                "loras": ("WAVESPEED_FLUX_LORAS", {"tooltip": "List of LoRA models to apply (max 5 items)"}),
                "width": ("INT", {
                    "default": 1024,
                    "min": 512,
                    "max": 1536,
                    "step": 8,
                    "display": "number",
                    "tooltip": "Image width (512 to 1536)"
                }),
                "height": ("INT", {
                    "default": 1024,
                    "min": 512,
                    "max": 1536,
                    "step": 8,
                    "display": "number",
                    "tooltip": "Image height (512 to 1536)"
                }),
                "num_inference_steps": ("INT", {
                    "default": 28,
                    "min": 1,
                    "max": 50,
                    "step": 1,
                    "display": "number",
                    "tooltip": "Number of inference steps for dev models (1 to 50)"
                }),
                "seed": ("INT", {
                    "default": -1,
                    "min": -1,
                    "max": 0xffffffffffffffff,
                    "control_after_generate": True,
                    "tooltip": "Random seed for reproducible results. -1 for random seed"
                }),
                "guidance_scale": ("FLOAT", {
                    "default": 5.0,
                    "min": 0.0,
                    "max": 10.0,
                    "step": 0.1,
                    "display": "number",
                    "tooltip": "Guidance scale for generation (0.0 to 10.0)"
                }),
                "num_images": ("INT", {
                    "default": 1,
                    "min": 1,
                    "max": 4,
                    "step": 1,
                    "display": "number",
                    "tooltip": "Number of images to generate (1 to 4)"
                }),
                "enable_safety_checker": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "Enable safety checker for generated content"
                }),
            }
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image", )

    FUNCTION = "generate"

    CATEGORY = "WaveSpeedAI"

    def generate(self,
                 client,
                 model,
                 image_url,
                 strength,
                 prompt="",
                 loras=None,
                 width=1024,
                 height=1024,
                 num_inference_steps=28,
                 seed=-1,
                 guidance_scale=5.0,
                 num_images=1,
                 enable_safety_checker=True):
        """
        Generate images using the Flux model

        Args:
            client: WaveSpeed AI API client
            model: Model name
            image_url: Input image for image-to-image generation
            strength: Strength of image-to-image transformation (0.0 to 1.0)
            prompt: Text prompt
            loras: List of LoRA models (max 5)
            width: Image width (512 to 1536)
            height: Image height (512 to 1536)
            num_inference_steps: Number of inference steps (1 to 50)
            seed: Random seed, -1 for random seed
            guidance_scale: Generation guidance scale (0.0 to 10.0)
            num_images: Number of images to generate (1 to 4)
            enable_safety_checker: Whether to enable safety checker

        Returns:
            tuple: (Generated image)
        """
        try:
            response = client.flux_generate_image(
                model=model,
                prompt=prompt,
                image=image_url,  
                strength=strength,
                loras=loras,
                width=width,
                height=height,
                num_inference_steps=num_inference_steps,
                seed=seed,
                guidance_scale=guidance_scale,
                num_images=num_images,
                enable_safety_checker=enable_safety_checker,
                wait_for_completion=True
            )
            # Download and process images
            image_urls = response.get("image_urls", [])
            if not image_urls:
                raise ValueError("No image URLs in the generated result")

            images = []
            for url in image_urls:
                print(f'WaveSpeed AI output: {url}')
                image_data = _fetch_image(url)
                image = _decode_image(image_data)
                images.append(image)

            # Convert to tensor
            tensor = _images2tensor(images)
            return (tensor,)
        except Exception as e:
            print(f"Generation failed: {str(e)}")
            raise e


class FluxText2Image:
    """
    Flux Image Generator Node

    This node uses WaveSpeed AI's Flux model to generate high-quality images.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "client": ("WAVESPEED_AI_API_CLIENT",),
                "model": (["flux-dev", "flux-schnell"], {"tooltip": "Model name, choose from flux-dev or flux-schnell"}),
                "prompt": ("STRING", {"multiline": True, "default": "", "tooltip": "Text description of the image to generate"}),
            },
            "optional": {
                "loras": ("WAVESPEED_FLUX_LORAS", {"tooltip": "List of LoRA models to apply (max 5 items)"}),
                "width": ("INT", {
                    "default": 1024,
                    "min": 512,
                    "max": 1536,
                    "step": 8,
                    "display": "number",
                    "tooltip": "Image width (512 to 1536)"
                }),
                "height": ("INT", {
                    "default": 1024,
                    "min": 512,
                    "max": 1536,
                    "step": 8,
                    "display": "number",
                    "tooltip": "Image height (512 to 1536)"
                }),
                "num_inference_steps": ("INT", {
                    "default": 28,
                    "min": 1,
                    "max": 50,
                    "step": 1,
                    "display": "number",
                    "tooltip": "Number of inference steps for dev models (1 to 50)"
                }),
                 "seed": ("INT", {
                    "default": -1,
                    "min": -1,
                    "max": 0xffffffffffffffff,
                    "control_after_generate": True,
                    "tooltip": "Random seed for reproducible results. -1 for random seed"
                }),
                "guidance_scale": ("FLOAT", {
                    "default": 5.0,
                    "min": 0.0,
                    "max": 10.0,
                    "step": 0.1,
                    "display": "number",
                    "tooltip": "Guidance scale for generation (0.0 to 10.0)"
                }),
                "num_images": ("INT", {
                    "default": 1,
                    "min": 1,
                    "max": 4,
                    "step": 1,
                    "display": "number",
                    "tooltip": "Number of images to generate (1 to 4)"
                }),
                "enable_safety_checker": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "Enable safety checker for generated content"
                }),
            }
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image", )

    FUNCTION = "generate"

    CATEGORY = "WaveSpeedAI"

    def generate(self,
                 client,
                 model,
                 prompt,
                 loras=None,
                 width=1024,
                 height=1024,
                 num_inference_steps=28,
                 seed=-1,
                 guidance_scale=5.0,
                 num_images=1,
                 enable_safety_checker=True):
        """
        Generate images using the Flux model

        Args:
            client: WaveSpeed AI API client
            model: Model name
            prompt: Text prompt
            loras: List of LoRA models (max 5)
            width: Image width (512 to 1536)
            height: Image height (512 to 1536)
            num_inference_steps: Number of inference steps (1 to 50)
            seed: Random seed, -1 for random seed
            guidance_scale: Generation guidance scale (0.0 to 10.0)
            num_images: Number of images to generate (1 to 4)
            enable_safety_checker: Whether to enable safety checker

        Returns:
            tuple: (Generated image,)
        """
        try:
            response = client.flux_generate_image(
                model=model,
                prompt=prompt,
                loras=loras,
                width=width,
                height=height,
                num_inference_steps=num_inference_steps,
                seed=seed,
                guidance_scale=guidance_scale,
                num_images=num_images,
                enable_safety_checker=enable_safety_checker,
                wait_for_completion=True
            )

            # Download and process images
            image_urls = response.get("image_urls", [])
            if not image_urls:
                raise ValueError("No image URLs in the generated result")

            images = []
            for url in image_urls:
                print(f'WaveSpeed AI output: {url}')
                image_data = _fetch_image(url)
                image = _decode_image(image_data)
                images.append(image)

            # Convert to tensor
            tensor = _images2tensor(images)
            return (tensor,)
        except Exception as e:
            print(f"Generation failed: {str(e)}")
            raise e

class WanText2VideoNode:
    """
    Wan Text to Video Node

    This node uses WaveSpeed AI's Wan T2V model to generate videos from text prompts.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "client": ("WAVESPEED_AI_API_CLIENT",),
                "model": (["wan-2.1/t2v-720p", "wan-2.1/t2v-480p","wan-2.1/t2v-720p-ultra-fast", "wan-2.1/t2v-480p-ultra-fast"],),
                "prompt": ("STRING", {"multiline": True, "default": "", "tooltip": "Text prompt to guide video generation"}),
            },
            "optional": {
                "negative_prompt": ("STRING", {"multiline": True, "default": "", "tooltip": "Negative prompt to specify what to avoid in the video"}),
                "size": (["None","720*1280", "1280*720", "480*832", "832*480"], {"tooltip": "Video dimensions in width x height format. 480p: 832*480 or 480*832, 720p: 1280*720 or 720*1280"}),
                "loras": ("WAVESPEED_WAN_LORAS", {"tooltip": "List of LoRA models to apply (maximum 3)"}),
                "num_inference_steps": ("INT", {
                    "default": 30,
                    "min": 1,
                    "max": 40,
                    "step": 1,
                    "display": "number",
                    "tooltip": "Number of inference steps"
                }),
                "guidance_scale": ("FLOAT", {
                    "default": 5.0,
                    "min": 0.0,
                    "max": 10.0,
                    "step": 0.1,
                    "display": "number",
                    "tooltip": "Guidance scale for generation"
                }),
                "seed": ("INT", {
                    "default": -1,
                    "min": -1,
                    "max": 0xffffffffffffffff,
                    "control_after_generate": True,
                    "tooltip": "Random seed for reproducible results. -1 for random seed"
                }),
                "enable_safety_checker": ("BOOLEAN", {"default": True, "tooltip": "Enable safety checker for generated content"}),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("url",)

    FUNCTION = "generate"

    CATEGORY = "WaveSpeedAI"

    def generate(self,
                 client,
                 model,
                 prompt,
                 negative_prompt="",
                 size="",
                 loras=None,
                 num_inference_steps=30,
                 guidance_scale=5.0,
                 seed=-1,
                 enable_safety_checker=True):
        """
        Generate video from text prompt

        Args:
            client: WaveSpeed AI API client
            model: Model name
            prompt: Text prompt to guide video generation
            negative_prompt: Negative prompt to specify what to avoid in the video
            size: Video dimensions in width*height format
            loras: List of LoRA models to apply (max 3)
            num_inference_steps: Number of inference steps
            guidance_scale: Guidance scale for generation
            seed: Random seed for reproducible results. -1 for random seed
            enable_safety_checker: Enable safety checker for generated content

        Returns:
            tuple: (video_url,)
        """
        if size == "None":
            size = None
        try:
            response = client.wan_text_to_video(
                model=model,
                prompt=prompt,
                negative_prompt=negative_prompt,
                loras=loras,
                size=size,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                seed=seed,
                enable_safety_checker=enable_safety_checker,
                wait_for_completion=True
            )

            video_url = response.get("video_url", "")

            return (video_url,)
        except Exception as e:
            print(f"Generation failed: {str(e)}")
            raise e

class WanImage2VideoNode:
    """
    Wan Image to Video Node

    This node uses WaveSpeed AI's Wan I2V model to generate videos from image.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "client": ("WAVESPEED_AI_API_CLIENT",),
                "model": (["wan-2.1/i2v-720p", "wan-2.1/i2v-480p","wan-2.1/i2v-720p-ultra-fast", "wan-2.1/i2v-480p-ultra-fast"],),
                "image_url": ("STRING", {"multiline": False, "default": "", "tooltip": "Image URL for video generation"}),
                "prompt": ("STRING", {"multiline": True, "default": "", "tooltip": "Text prompt to guide video generation"}),
            },
            "optional": {
                "negative_prompt": ("STRING", {"multiline": True, "default": "", "tooltip": "Negative prompt to specify what to avoid in the video"}),
                "size": (["None","720*1280", "1280*720", "480*832", "832*480"], {"tooltip": "Video dimensions in width x height format. 480p: 832*480 or 480*832, 720p: 1280*720 or 720*1280"}),
                "loras": ("WAVESPEED_WAN_LORAS", {"tooltip": "List of LoRA models to apply (maximum 3)"}),
                "num_inference_steps": ("INT", {
                    "default": 30,
                    "min": 1,
                    "max": 40,
                    "step": 1,
                    "display": "number",
                    "tooltip": "Number of inference steps"
                }),
                "guidance_scale": ("FLOAT", {
                    "default": 5.0,
                    "min": 0.0,
                    "max": 10.0,
                    "step": 0.1,
                    "display": "number",
                    "tooltip": "Guidance scale for generation"
                }),
                "seed": ("INT", {
                    "default": -1,
                    "min": -1,
                    "max": 0xffffffffffffffff,
                    "control_after_generate": True,
                    "tooltip": "Random seed for reproducible results. -1 for random seed"
                }),
                "enable_safety_checker": ("BOOLEAN", {"default": True, "tooltip": "Enable safety checker for generated content"}),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("url",)

    FUNCTION = "generate"

    CATEGORY = "WaveSpeedAI"

    def generate(self,
                 client,
                 model,
                 image_url,
                 prompt,
                 negative_prompt="",
                 size="",
                 loras=None,
                 num_inference_steps=30,
                 guidance_scale=5.0,
                 seed=-1,
                 enable_safety_checker=True):
        """
        Generate video from image and text prompt

        Args:
            client: WaveSpeed AI API client
            model: Model name
            image_url: Input image
            prompt: Text prompt to guide video generation
            negative_prompt: Negative prompt to specify what to avoid in the video
            size: Video dimensions in width*height format
            loras: List of LoRA models to apply (max 3)
            num_inference_steps: Number of inference steps
            guidance_scale: Guidance scale for generation
            seed: Random seed for reproducible results. -1 for random seed
            enable_safety_checker: Enable safety checker for generated content

        Returns:
            tuple: (video_url,)
        """
        try:
            if size == "None":
                size = None
            response = client.wan_image_to_video(
                model=model,
                image=image_url,
                prompt=prompt,
                negative_prompt=negative_prompt,
                loras=loras,
                size=size,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                seed=seed,
                enable_safety_checker=enable_safety_checker,
                wait_for_completion=True
            )

            video_url = response.get("video_url", "")

            return (video_url,)
        except Exception as e:
            print(f"Generation failed: {str(e)}")
            raise e


class MinimaxImage2VideoNode:
    """
    Minimax Image to Video Node

    This node uses Minimax AI's Image to Video model to generate videos from images.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "client": ("WAVESPEED_AI_API_CLIENT",),
                "model": (["minimax/video-01"],),
                "first_frame_image": ("STRING", {"multiline": False, "default": "", "tooltip": "URL of the image to use as the first frame of the video"}),
                "prompt": ("STRING", {"multiline": True, "default": "", "tooltip": "Text description of the video content to generate"}),
            },
            "optional": {
                "prompt_optimizer": ("BOOLEAN", {"default": False, "tooltip": "Whether to automatically optimize the prompt for better results"}),
                "subject_reference": ("STRING", {"multiline": False, "default": "", "tooltip": "URL of an image containing the subject to reference for consistent appearance"}),
            }
        }

    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("video_url", "task_id")

    FUNCTION = "generate"

    CATEGORY = "MinimaxAI"

    def generate(self,
                 client,
                 model,
                 first_frame_image,
                 prompt,
                 prompt_optimizer=False,
                 subject_reference=""):
        """
        Generate video from image and text prompt

        Args:
            client: Minimax AI API client
            model: Model name
            first_frame_image: URL of the image to use as the first frame of the video
            prompt: Text description of the video content to generate
            prompt_optimizer: Whether to automatically optimize the prompt for better results
            subject_reference: URL of an image containing the subject to reference for consistent appearance

        Returns:
            tuple: (video_url, task_id)
        """
        try:
            response = client.minimax_image_to_video(
                model=model,
                first_frame_image=first_frame_image,
                prompt=prompt,
                prompt_optimizer=prompt_optimizer,
                subject_reference=subject_reference,
                wait_for_completion=True
            )

            video_url = response.get("video_url", "")
        
            return (video_url,)
        except Exception as e:
            print(f"Generation failed: {str(e)}")
            raise e